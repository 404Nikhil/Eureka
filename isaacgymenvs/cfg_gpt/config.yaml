defaults:
  - _self_
  # - env: shadow_hand_kettle
  - env: shadow_hand_over
  # - example: cartpole # which example to show in prompt
  - override hydra/launcher:  local
  - override hydra/output:  local

suffix: GPT
# model: gpt-4
# model: gpt-4-0613
model: gpt-4-0314
# model: gpt-3.5-turbo-16k-0613
temperature: 1.0
checkpoint: '' # path of policy initialization (used in gpt_rlhf and gpt_curriculum)

sample: 3 # number of RewardCoder samples to generate
iteration: 1 # How many rounds of RewardCoder to run
max_iterations: 3000 # RL Policy training iterations (decrease this to make the feedback loop faster)
num_eval: 5 # number of evaluation episodes to run for the final reward

paraphrase: -1 # whether to to paraphrase key feedback 
reward_criterion: "max" # Criterion for selecting the best RewardGPT sample in an iteration
gt_criterion: True # whether to provide the ground-truth task criterion as feedback
use_wandb: False # whether to use wandb for logging
test_rl: False # whether to just test out functionalities related to RL training in subprocesses
text_only: False # whether to enter purely chat mode and skip policy training
capture_video: False # whether to capture policy rollout videos
self_critic: False # whether to use human textual feedback (before the policy feedback loop)
human: False # whether to use human textual feedback (within the policy feedback loop)
no_verbalization: False # whether to not use policy verbalization as feedback

load_message: "" # can specify a path to load message history
original_path: ""
